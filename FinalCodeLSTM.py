# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S3CSAkgg76CQkTHCd27MG6yYy35ADZ6G
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz

!tar xf spark-3.4.0-bin-hadoop3.tgz

!pwd
!ls -l

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.0-bin-hadoop3"

!pip install -q findspark
import findspark

findspark.init()

from pyspark.sql import SparkSession 
spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

import torch
import torch.nn as nn

import functools
import sys
import pandas

from torch import optim
import torchtext
s3url = "https://bigdataproject-s23.s3.amazonaws.com/collectivetweets.csv"
!wget https://bigdataproject-s23.s3.amazonaws.com/collectivetweets.csv
!ls

tweetdf = spark.read.option("header", "true").option("inferSchema", "true").csv("collectivetweets.csv")

from pyspark.ml.feature import *

from nltk.tokenize import *
import re
from nltk.corpus import stopwords
from pyspark.sql.functions import *

def preProcessing(tweetLabel):
    tweet = tweetLabel[0]
    if tweet == None or len(tweet.split(" ")) == 0:
        return (" ", tweetLabel[1])
    
    # removing username from input sentence
    string = ' '.join(re.sub("(@[A-Za-z0-9]+)"," ",tweet).split())
 
    # removing hashtags from input sentence
    string1 = ' '.join(re.sub("([^0-9A-Za-z \t])"," ",string).split())
 
    # removing special characters from input sentence
    string2 = ' '.join(re.sub("(\w+:\/\/\S+)"," ",string1).split()).lower()
 
    return (string2,tweetLabel[1])

tweetdf = tweetdf.withColumnRenamed("Tweet","text").withColumnRenamed("Polarity","label")
tweetdf = tweetdf.withColumn("label", when(tweetdf.label < 0, 0).otherwise(1))

tweet_rdd = tweetdf.rdd.map(lambda x: (x["text"],x["label"]))
tweet_rdd_preprocessed = tweet_rdd.map(lambda x:preProcessing(x))
 
tweet_df_preprocessed = spark.createDataFrame(tweet_rdd_preprocessed, schema = ["text", "label"])

tweet_df_preprocessed.printSchema()

regexTokenizer = RegexTokenizer(inputCol="text", outputCol="words", pattern="\\W")
regexTokenized_tweets = regexTokenizer.transform(tweet_df_preprocessed)
regexTokenized_tweets.show(10)

remover = StopWordsRemover(inputCol="words", outputCol="filtered")
tweets_regexed = remover.transform(regexTokenized_tweets)

tweets_renamed = tweets_regexed.withColumnRenamed("filtered","tokens").filter(size('tokens')>0).select('tokens', 'label')
string_lengths = transform(array('tokens'), lambda x: length(x))
tweets_df = tweets_renamed.select(col('tokens'), col('label'))
tweets_df = tweets_df.withColumn("length", size("tokens"))
train_df, test_df = tweets_df.select('tokens','label','length').randomSplit([0.7,0.3],seed=100)

test_size = 0.20

!pip install --upgrade pyarrow

!pip install datasets

test_df.printSchema()

from datasets import Dataset, Features, Value
from datasets import Features, Value

features = Features({
    "tokens": Value("string", id="tokens_id" ),
    "length": Value("int32", id="length_id"),
    "label": Value("int64", id="label_id"),
})

rows = test_df.rdd.map(lambda x: {"tokens": x.tokens, "length": x.length, "label": x.label})

from pyspark.sql.functions import concat_ws
test_df = test_df.withColumn("tokens_str", concat_ws(",", "tokens")).drop("tokens")

train_df = train_df.withColumn("tokens_str", concat_ws(",", "tokens")).drop("tokens")

test_df.printSchema()
train_df.printSchema()

!ls -l train1.csv/

train_df.coalesce(1).write.mode("overwrite").format("csv").option("header", "true").save("train1.csv")
test_df.coalesce(1).write.mode("overwrite").format("csv").option("header", "true").save("test1.csv")

!mv test1.csv/*.csv test.csv

!mv train1.csv/*.csv train.csv

from datasets import load_dataset
testdataset = load_dataset("csv", data_files="test.csv")

traindataset = load_dataset("csv", data_files="train.csv")

def tokenize(example):
    example['tokens'] = example['tokens_str'].split(',')
    return example

tdataset = testdataset.map(tokenize)

trdataset = traindataset.map(tokenize)

f1dataset = tdataset.remove_columns(['tokens_str'])

ftrdataset = trdataset.map(tokenize)



my_test_dataset = f1dataset

trd1ataset = tdataset.remove_columns(['tokens_str'])
my_train_dataset = trd1ataset

my_train_dataset

my_test_dataset





train_data = my_train_dataset['train']
test_data = my_test_dataset['train']

tv_split = train_data.train_test_split(test_size=0.3)

tv_split

train_data = tv_split['train']
valid_data = tv_split['test']

from torchtext.vocab import build_vocab_from_iterator as bv, FastText
from torch.nn import Linear as Ln
from torch.nn import  Dropout as Dr
from torch.nn.utils.rnn import  pack_padded_sequence as pps

tk_to_skip = ['<unk>', '<pad>']
vocabulary_of_words = bv(train_data['tokens'], min_freq=4, specials=tk_to_skip)

ui = vocabulary_of_words[tk_to_skip[0]]
pi = vocabulary_of_words[tk_to_skip[1]]

print(pi)

vocabulary_of_words.set_default_index(ui)

def count_vectorize(e):
    return {'cv': [vocabulary_of_words[token] for token in e['tokens']]}

ftype_name = 'torch'
tr_d = train_data.map(count_vectorize)
v_d = valid_data.map(count_vectorize)
te_d = test_data.map(count_vectorize)

tr_d = tr_d.with_format(type=ftype_name, columns=['cv', 'label', 'length'])
v_d = v_d.with_format(type=ftype_name, columns=['cv', 'label', 'length'])
te_d = te_d.with_format(type=ftype_name, columns=['cv', 'label', 'length'])

class CustomLSTM(nn.Module):
    def __init__(self, vs, d_em, hidden_dim, d_op, ly, bid, dr, pi):
        super().__init__()
        b_f = True
        self.e = nn.Embedding(vs, d_em, padding_idx=pi)
        self.ls = nn.LSTM(d_em, hidden_dim, ly, bidirectional=bid, dropout=dr, batch_first=b_f)
        self.fc = Ln(hidden_dim * 2 if bid else hidden_dim, d_op)
        self.dr = Dr(dr)
        
    def forward(self, id, l):
        b_f = True
        e_s = False
        bid_value = self.ls.bidirectional
        p_e = pps(self.dr(self.e(id)), l, batch_first=b_f, enforce_sorted=e_s)
        p_o, (hn, cell) = self.ls(p_e)
        o, o_l = nn.utils.rnn.pad_packed_sequence(p_o)
        if not bid_value:
            hn = self.dr(hn[-1])
        else:
            hn = self.dr(torch.cat([hn[-1], hn[-2]], dim=-1))
        return self.fc(hn)

model = CustomLSTM(len(vocabulary_of_words), 300, 300, len(train_data.unique('label')), 2, True, 0.5, pi)

def app_wegh(_model):
    params = _model.named_parameters()
    b = 'bias'
    w = 'weight'
    for n, p in params:
        if b in n:
            nn.init.zeros_(p)
        elif w in n:
            nn.init.orthogonal_(p)

model.apply(app_wegh)

vocab_vectors = FastText()

train_embed = vocab_vectors.get_vecs_by_tokens(vocabulary_of_words.get_itos())

model.e.weight.data = train_embed

optimizer = optim.Adam(model.parameters(), lr=5e-4)

from torch.nn import CrossEntropyLoss as ce
criterion = ce()
perf_conf = torch.device('cpu')

model = model.to(perf_conf)
crt = criterion.to(perf_conf)

from torch.nn.utils.rnn import pad_sequence as ps
def colt(bt, pi):
    b_f = True
    b_ids = [i['cv'] for i in bt]
    b_ids = ps(b_ids, padding_value=pi, batch_first=b_f)
    b_l = [i['length'] for i in bt]
    b_l = torch.stack(b_l)
    b_lb = [i['label'] for i in bt]
    b_lb = torch.stack(b_lb)
    return {'cv': b_ids,'length': b_l,'label': b_lb}

from torch.utils.data import DataLoader as DL
from functools import partial as pt
clf = pt(colt, pi=pi)
v_bool_shuf = True
train_dataloader = DL(tr_d, batch_size=512, collate_fn=clf, shuffle=v_bool_shuf)
valid_dataloader = DL(v_d, batch_size=512, collate_fn=clf)
test_dataloader = DL(te_d, batch_size=512, collate_fn=clf)

def get_accuracy(p, l):
    b_s, _ = p.shape
    p_c = p.argmax(dim=-1)
    c_p = p_c.eq(l).sum()
    return c_p / b_s

from tqdm import tqdm as progress
def train(dl, md, cr, o, d):

    md.train()
    e_l = []
    e_a = []

    for bt in progress(dl, desc='under process', file=sys.stdout):
        ids = bt['cv'].to(d)
        lt = bt['length']
        lb = bt['label'].to(d)
        ls = cr(md(ids, lt), lb)
        _a = get_accuracy(md(ids, lt), lb)
        o.zero_grad()
        ls.backward()
        o.step()
        e_l.append(ls.item())
        e_a.append(_a.item())

    return e_l, e_a

consoleloc = sys.stdout 
def evaluate(dl, md, cr, d):
    
    md.eval()
    e_l = []
    e_a = []

    with torch.no_grad():
        for bt in progress(dl, desc='calculating', file=consoleloc):
            ids = bt['cv'].to(d)
            lt = bt['length']
            lb = bt['label'].to(d)
            p = md(ids, lt)
            ls = cr(p, lb)
            _a = get_accuracy(p, lb)
            e_l.append(ls.item())
            e_a.append(_a.item())

    return e_l, e_a

import numpy as np
from numpy import mean as avg

testloops = 10
b_v_l = float('inf')

tr_l = []
tr_a = []
v_l = []
v_a = []

for loop in range(testloops):

    loss_of_t, accuracy_of_train = train(train_dataloader, model, crt, optimizer, torch.device('cpu'))
    loss_of_v, accuracy_of_validation = evaluate(valid_dataloader, model, crt, torch.device('cpu'))

    tr_l.extend(loss_of_t)
    tr_a.extend(accuracy_of_train)
    v_l.extend(loss_of_v)
    v_a.extend(accuracy_of_validation)
    
    etl = avg(loss_of_t)
    eta = avg(accuracy_of_train)
    evl = avg(loss_of_v)
    eva = avg(accuracy_of_validation)
    
    if evl < b_v_l:
        b_v_l = evl
        torch.save(model.state_dict(), 'lstm.pt')
    
    print(f'Accuracy of Training: {eta:.3f}')
    print(f'Accuracy in validation: {eva:.3f}')

import nltk
nltk.download('punkt')
tok = torchtext.data.utils.get_tokenizer('basic_english')

from torch import LongTensor as LT
def predict_sentiment(text, model, vocab, device):
    ids = [vocab[t] for t in tok(text)]
    l = LT([len(ids)])
    t = LT(ids).unsqueeze(dim=0).to(device)
    prd = model(t, l).squeeze(dim=0)
    prob = torch.softmax(prd, dim=-1)
    pr_cl = prd.argmax(dim=-1).item()
    return pr_cl, prob[pr_cl].item()

text = "This film is worst"

_, polarity = predict_sentiment(text, model, vocabulary_of_words, torch.device('cpu'))
print(polarity)

!ls -l

#set secrets as env variables, dont share this code
import os
os.environ["news_api_ai_key"] = "76875077-d9ee-4bdb-8f53-da936f099ce9"
os.environ["news_api_app_id"] = "53988099"
os.environ["news_api_app_key"] = "74f5989a6f8bc71efc3970ef6a289f6f"
os.environ["news_api_org"] = "f401be411dec4ea69aa09a0d60abee14"

!pip install newsapi-python

import os
from newsapi import NewsApiClient
import requests
import json
import urllib.parse

news_api_id = os.getenv("news_api_app_id")
news_api_key = os.getenv("news_api_app_key")
news_api_url = "https://api.aylien.com/news/stories?text="
news_api_header = {
    "X-AYLIEN-NewsAPI-Application-ID": news_api_id,
    "X-AYLIEN-NewsAPI-Application-Key": news_api_key
}


news_api_ai_key = os.getenv("news_api_ai_key")
news_api_ai_key_url = "http://eventregistry.org/api/v1/article/getArticles"
news_api_ai_key_body = {
    "action": "getArticles",
    "keyword": "leader_name",
    "articlesPage": 1,
    "articlesCount": 10,
    "articlesSortBy": "date",
    "articlesSortByAsc": False,
    "articlesArticleBodyLen": -1,
    "resultType": "articles",
    "dataType": [
        "news",
        "pr"
    ],
    "apiKey": news_api_ai_key,
    "forceMaxDataTimeWindow": 31
}


news_api_org = os.getenv("news_api_org")

news_api_set = set()
news_api_ai_set = set()
news_api_org_set = set()

political_leaders = ["narendra modi", "joe biden", "donald trump", "emmanuel macron",
                     "kamala harris", "xi jinping", "kim jong un", "rishi sunak", "ron desantis"]

!pip install kafka-python

from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers=['ec2-3-83-29-213.compute-1.amazonaws.com:9092'],
    value_serializer=lambda m: str(m).encode('utf-8'))

newsapi = NewsApiClient(api_key=news_api_org)


for leader in political_leaders:
    news = []

    top_headlines = newsapi.get_everything(q=leader,
                                           sources='bbc-news,the-verge',
                                           language='en')
    for i in range(len(top_headlines)):
        if not top_headlines['articles'][i]['title'] in news_api_org_set:
            news_api_org_set.add(top_headlines['articles'][i]['title'])
            news.append(top_headlines['articles'][i]['title']+" "+top_headlines['articles'][i]['description'])
    news_api_ai_key_body["keyword"] = leader
    response =  requests.post(news_api_ai_key_url, json=news_api_ai_key_body)
    newsobj =  json.loads(response.text)

    for i in range(len(newsobj)):
        if not newsobj['articles']['results'][i]['uri'] in news_api_ai_set:
            news_api_ai_set.add(newsobj['articles']['results'][i]['uri'])
            news.append(newsobj['articles']['results'][i]['title'])

    url_name = urllib.parse.quote(leader)
    news_api_url_leader = news_api_url + url_name +"&language=en"

    response = requests.get(news_api_url_leader, headers= news_api_header)
    news_api_obj = json.loads(response.text)
    for i in range(len(news_api_obj)):
        if not news_api_obj['stories'][i]['id'] in news_api_set:
            news_api_set.add(news_api_obj['stories'][i]['id'])
            news.append(news_api_obj['stories'][i]['title'])

    for _n in news:
      _, polarity = predict_sentiment(_n, model, vocabulary_of_words, torch.device('cpu'))
      print(_n, polarity)
      item_dict = {"news": _n, "leader":leader, "lstmpolarity" : polarity}
      json_string = json.dumps(item_dict)
      print("Sending...", json_string)
      producer.send('my-topic', json_string)



!ls

